{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1358e2-97bc-411d-bc5b-80e347f9963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5431680-a333-4952-8499-07e257f82f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/minghui-liu/cold-compress.git\n",
    "cd cold-compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca0c3a-f27c-4e9c-8091-292db61a4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -r requirements.txt --extra-index-url https://download.pytorch.org/whl/nightly/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448aa14-0ab2-46a6-a62c-13e848081c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "\n",
    "!echo \"HUGGINGFACE_TOKEN=[]\" > .env\n",
    "!echo \"OPENAI_API_KEY=[]\" >> .env\n",
    "!cat .env\n",
    "!export $(grep -v '^#' .env | xargs -d '\\n')\n",
    "!huggingface-cli login --token [your token here] --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842eee80-e2dc-46e8-80d9-c07241107b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash scripts/prepare_llama31.sh\n",
    "!bash scripts/prepare_qwen2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fec1a-59a3-49ac-81f5-3a9d8e920875",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7984e9-ada5-454f-9986-0c36ce2dd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash prepare_data.sh qwen2-7b-chat synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecb5c1a-0b98-4c3e-8b8c-b9bfadf59e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['niah_single_1_131k', 'niah_single_1_64k', 'niah_single_1_32k', 'niah_single_1_16k', 'niah_single_1_8k', 'niah_single_2_131k', 'niah_single_2_64k', 'niah_single_2_32k', 'niah_single_2_16k', 'niah_single_2_8k', 'niah_single_3_131k', 'niah_single_3_64k', 'niah_single_3_32k', 'niah_single_3_16k', 'niah_single_3_8k', 'niah_multikey_1_131k', 'niah_multikey_1_64k', 'niah_multikey_1_32k', 'niah_multikey_1_16k', 'niah_multikey_1_8k', 'niah_multikey_2_131k', 'niah_multikey_2_64k', 'niah_multikey_2_32k', 'niah_multikey_2_16k', 'niah_multikey_2_8k', 'niah_multikey_3_131k', 'niah_multikey_3_64k', 'niah_multikey_3_32k', 'niah_multikey_3_16k', 'niah_multikey_3_8k', 'niah_multivalue_131k', 'niah_multivalue_64k', 'niah_multivalue_32k', 'niah_multivalue_16k', 'niah_multivalue_8k', 'niah_multiquery_131k', 'niah_multiquery_64k', 'niah_multiquery_32k', 'niah_multiquery_16k', 'niah_multiquery_8k', 'vt_131k', 'vt_64k', 'vt_32k', 'vt_16k', 'vt_8k', 'cwe_131k', 'cwe_64k', 'cwe_32k', 'cwe_16k', 'cwe_8k', 'fwe_131k', 'fwe_64k', 'fwe_32k', 'fwe_16k', 'fwe_8k', 'qa_1_131k', 'qa_1_64k', 'qa_1_32k', 'qa_1_16k', 'qa_1_8k', 'qa_2_131k', 'qa_2_64k', 'qa_2_32k', 'qa_2_16k', 'qa_2_8k'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 57.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  5.06 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  8.37ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  4.90 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.55ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.98 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.57ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.49 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 21.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  4.84 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 27.78ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  8.38 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.30ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.02 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.59ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.12 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 58.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.89 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 14.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.09 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 34.38ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.17 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  8.93ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.06 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 10.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.17 shards/s]\n"
     ]
    }
   ],
   "source": [
    "######### Create RULER dataset of various length #########\n",
    "from __future__ import annotations\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import Any, List, Union\n",
    "\n",
    "def read_json_objects(file_path: Union[str, os.PathLike]) -> Union[Any, List[Any]]:\n",
    "    \"\"\"\n",
    "    Read a JSON/JSONL file and return its contents.\n",
    "\n",
    "    - If `file_path` ends with .json -> returns the JSON object (dict, list, etc.).\n",
    "    - If `file_path` ends with .jsonl or .ndjson -> returns a list of JSON objects, one per line.\n",
    "    - If the extension is ambiguous, it first tries to `json.load`; on failure it\n",
    "      falls back to parsing it as JSON Lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str | os.PathLike\n",
    "        Path to the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any | list[Any]\n",
    "        A JSON object for .json files, or a list of JSON objects for .jsonl/.ndjson.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file does not exist.\n",
    "    json.JSONDecodeError\n",
    "        If the file cannot be decoded as JSON or JSON Lines.\n",
    "    \"\"\"\n",
    "    path_str = os.fspath(file_path)\n",
    "    is_jsonl = path_str.endswith((\".jsonl\", \".ndjson\"))\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if is_jsonl:\n",
    "            return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "        # Try standard JSON first\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fall back to JSON Lines parsing\n",
    "            f.seek(0)\n",
    "            try:\n",
    "                return [json.loads(line) for line in f if line.strip()]\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Re-raise with the original context\n",
    "                raise e \n",
    "\n",
    "######## llama model ########\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tasks = [\n",
    "    \"niah_single_1\",\n",
    "    \"niah_single_2\",\n",
    "    \"niah_single_3\",\n",
    "    \"niah_multikey_1\",\n",
    "    \"niah_multikey_2\",\n",
    "    \"niah_multikey_3\",\n",
    "    \"niah_multivalue\",\n",
    "    \"niah_multiquery\",\n",
    "    \"vt\",\n",
    "    \"cwe\",\n",
    "    \"fwe\",\n",
    "    \"qa_1\",\n",
    "    \"qa_2\"\n",
    "]\n",
    "\n",
    "seq_lengths = [131072, 65536, 32768, 16384, 8192]\n",
    "seq_len_names = {\n",
    "    131072: \"131k\",\n",
    "    65536: \"64k\",\n",
    "    32768: \"32k\",\n",
    "    16384: \"16k\",\n",
    "    8192: \"8k\",\n",
    "}\n",
    "\n",
    "model_name = 'llama3.1-8b-chat'\n",
    "benchmark_root = '/home/ubuntu/hashevict/RULER/scripts/benchmark_root'\n",
    "data_root = os.path.join(benchmark_root, model_name, 'synthetic')\n",
    "\n",
    "my_dataset_dict = {}\n",
    "for task in tasks:\n",
    "    for seq_len in seq_lengths:\n",
    "        # create subset\n",
    "        subset_name = f\"{task}_{seq_len_names[seq_len]}\"\n",
    "        subset_json_file = os.path.join(data_root, str(seq_len), 'data', task, 'validation.jsonl')\n",
    "        subset_data = read_json_objects(subset_json_file)\n",
    "        my_dataset_dict[subset_name] = Dataset.from_list(subset_data)\n",
    "        \n",
    "print(my_dataset_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d9e4bb-89a5-4cb2-b7ca-911c6e79f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 256.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  6.74 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 105.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  7.52 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 102.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  7.80 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 117.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.30 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 146.18ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.70 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 211.97ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.46 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 114.91ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.10 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 120.15ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.87 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 217.41ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.28 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 110.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  4.14 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 187.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.53 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 104.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.57 shards/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 68.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.43 shards/s]\n"
     ]
    }
   ],
   "source": [
    "repo_id = 'minghuiliu/ruler_llama'\n",
    "for key in my_dataset_dict:\n",
    "    my_dataset_dict[key].push_to_hub(repo_id, config_name=key, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3d316-8a9d-4d23-9c50-3e91e698e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo_id = 'minghuiliu/ruler'\n",
    "for key in my_dataset_dict:\n",
    "    my_dataset_dict[key].push_to_hub(repo_id, config_name=key, split=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6363505-48d5-4e42-9c8a-58bc072146c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushed = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6155ac-6b90-4e4a-9b5d-60482ca110a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "######## qwen2 model ########\n",
    "model_name = 'qwen2-7b-chat'\n",
    "benchmark_root = '/home/ubuntu/hashevict/RULER/scripts/benchmark_root'\n",
    "data_root = os.path.join(benchmark_root, model_name, 'synthetic')\n",
    "\n",
    "my_dataset_dict = {}\n",
    "for task in tasks:\n",
    "    for seq_len in seq_lengths:\n",
    "        # create subset\n",
    "        subset_name = f\"{task}_{seq_len_names[seq_len]}\"\n",
    "        subset_json_file = os.path.join(data_root, str(seq_len), 'data', task, 'validation.jsonl')\n",
    "        subset_data = read_json_objects(subset_json_file)\n",
    "        my_dataset_dict[subset_name] = Dataset.from_list(subset_data)\n",
    "        \n",
    "print(my_dataset_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf5bea31-d622-4475-9224-4a8223dc7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo_id = 'minghuiliu/ruler_qwen'\n",
    "for key in my_dataset_dict:\n",
    "    if key not in pushed:\n",
    "        my_dataset_dict[key].push_to_hub(repo_id, config_name=key, split=\"validation\")\n",
    "        pushed.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcea34-0862-435c-96cf-d3c868acc36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
