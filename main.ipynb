{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1358e2-97bc-411d-bc5b-80e347f9963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5431680-a333-4952-8499-07e257f82f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/minghui-liu/cold-compress.git\n",
    "cd cold-compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca0c3a-f27c-4e9c-8091-292db61a4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -r requirements.txt --extra-index-url https://download.pytorch.org/whl/nightly/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448aa14-0ab2-46a6-a62c-13e848081c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "\n",
    "!echo \"HUGGINGFACE_TOKEN=[]\" > .env\n",
    "!echo \"OPENAI_API_KEY=[]\" >> .env\n",
    "!cat .env\n",
    "!export $(grep -v '^#' .env | xargs -d '\\n')\n",
    "!huggingface-cli login --token [your token here] --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842eee80-e2dc-46e8-80d9-c07241107b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash scripts/prepare_llama31.sh\n",
    "!bash scripts/prepare_qwen2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fec1a-59a3-49ac-81f5-3a9d8e920875",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7984e9-ada5-454f-9986-0c36ce2dd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash prepare_data.sh qwen2-7b-chat synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb5c1a-0b98-4c3e-8b8c-b9bfadf59e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Create RULER dataset of various length #########\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Any, List, Union\n",
    "\n",
    "def read_json_objects(file_path: Union[str, os.PathLike]) -> Union[Any, List[Any]]:\n",
    "    \"\"\"\n",
    "    Read a JSON/JSONL file and return its contents.\n",
    "\n",
    "    - If `file_path` ends with .json -> returns the JSON object (dict, list, etc.).\n",
    "    - If `file_path` ends with .jsonl or .ndjson -> returns a list of JSON objects, one per line.\n",
    "    - If the extension is ambiguous, it first tries to `json.load`; on failure it\n",
    "      falls back to parsing it as JSON Lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str | os.PathLike\n",
    "        Path to the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any | list[Any]\n",
    "        A JSON object for .json files, or a list of JSON objects for .jsonl/.ndjson.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file does not exist.\n",
    "    json.JSONDecodeError\n",
    "        If the file cannot be decoded as JSON or JSON Lines.\n",
    "    \"\"\"\n",
    "    path_str = os.fspath(file_path)\n",
    "    is_jsonl = path_str.endswith((\".jsonl\", \".ndjson\"))\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if is_jsonl:\n",
    "            return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "        # Try standard JSON first\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fall back to JSON Lines parsing\n",
    "            f.seek(0)\n",
    "            try:\n",
    "                return [json.loads(line) for line in f if line.strip()]\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Re-raise with the original context\n",
    "                raise e \n",
    "\n",
    "######## llama model ########\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tasks = [\n",
    "    \"niah_single_1\",\n",
    "    \"niah_single_2\",\n",
    "    \"niah_single_3\",\n",
    "    \"niah_multikey_1\",\n",
    "    \"niah_multikey_2\",\n",
    "    \"niah_multikey_3\",\n",
    "    \"niah_multivalue\",\n",
    "    \"niah_multiquery\",\n",
    "    \"vt\",\n",
    "    \"cwe\",\n",
    "    \"fwe\",\n",
    "    \"qa_1\",\n",
    "    \"qa_2\"\n",
    "]\n",
    "\n",
    "seq_lengths = [131072, 65536, 32768, 16384, 8192]\n",
    "seq_len_names = {\n",
    "    131072: \"131k\",\n",
    "    65536: \"64k\",\n",
    "    32768: \"32k\",\n",
    "    16384: \"16k\",\n",
    "    8192: \"8k\",\n",
    "}\n",
    "\n",
    "model_name = 'llama3.1-8b-chat'\n",
    "benchmark_root = '/home/ubuntu/hashevict/RULER/scripts/benchmark_root'\n",
    "data_root = os.path.join(benchmark_root, model_name, 'synthetic')\n",
    "\n",
    "my_dataset_dict = {}\n",
    "for task in tasks:\n",
    "    for seq_len in seq_lengths:\n",
    "        # create subset\n",
    "        subset_name = f\"{task}_{seq_len_names[seq_len]}\"\n",
    "        subset_json_file = os.path.join(data_root, str(seq_len), 'data', task, 'validation.jsonl')\n",
    "        subset_data = read_json_objects(subset_json_file)\n",
    "        my_dataset_dict[subset_name] = Dataset.from_list(subset_data)\n",
    "        \n",
    "print(my_dataset_dict.keys())\n",
    "\n",
    "repo_id = 'minghuiliu/ruler_llama'\n",
    "for key in my_dataset_dict:\n",
    "    my_dataset_dict[key].push_to_hub(repo_id, config_name=key, split=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813f486-4e76-47c6-af7d-7adb75c3aef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6155ac-6b90-4e4a-9b5d-60482ca110a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## qwen2 model ########\n",
    "model_name = 'qwen2-7b-chat'\n",
    "benchmark_root = '/home/ubuntu/hashevict/RULER/scripts/benchmark_root'\n",
    "data_root = os.path.join(benchmark_root, model_name, 'synthetic')\n",
    "\n",
    "my_dataset_dict = {}\n",
    "for task in tasks:\n",
    "    for seq_len in seq_lengths:\n",
    "        # create subset\n",
    "        subset_name = f\"{task}_{seq_len_names[seq_len]}\"\n",
    "        subset_json_file = os.path.join(data_root, str(seq_len), 'data', task, 'validation.jsonl')\n",
    "        subset_data = read_json_objects(subset_json_file)\n",
    "        my_dataset_dict[subset_name] = Dataset.from_list(subset_data)\n",
    "        \n",
    "print(my_dataset_dict.keys())\n",
    "\n",
    "repo_id = 'minghuiliu/ruler_qwen'\n",
    "for key in my_dataset_dict:\n",
    "    my_dataset_dict[key].push_to_hub(repo_id, config_name=key, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bea31-d622-4475-9224-4a8223dc7011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
